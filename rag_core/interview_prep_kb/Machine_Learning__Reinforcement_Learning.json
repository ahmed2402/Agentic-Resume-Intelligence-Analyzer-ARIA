[
  {
    "id": "q1",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Easy",
    "question": "What is the primary goal of a reinforcement learning agent?",
    "answer": "The primary goal of a reinforcement learning agent is to learn a policy that maps states to actions to maximize a reward signal over time."
  },
  {
    "id": "q2",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Easy",
    "question": "What is the difference between an MDP (Markov Decision Process) and a POMDP (Partially Observable Markov Decision Process)?",
    "answer": "A POMDP is an extension of an MDP where the state of the environment is not fully observable by the agent, whereas an MDP assumes full observability."
  },
  {
    "id": "q3",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Easy",
    "question": "What is epsilon-greedy, and how is it used in reinforcement learning?",
    "answer": "Epsilon-greedy is a exploration strategy where the agent chooses a random action with a probability of epsilon, and the greedy action with a probability of 1-epsilon. It is used to balance exploration and exploitation in reinforcement learning."
  },
  {
    "id": "q4",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Easy",
    "question": "What is Q-learning, and how does it work?",
    "answer": "Q-learning is a model-free reinforcement learning algorithm that learns the action-value function (Q-function) by iteratively updating the estimate of Q(s, a) using the Q-learning update rule: Q(s, a) ← Q(s, a) + alpha[r + gamma*max(Q(s', a')) - Q(s, a)]."
  },
  {
    "id": "q5",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Easy",
    "question": "What is Deep Q-Networks (DQN), and what is its main contribution?",
    "answer": "Deep Q-Networks (DQN) is a deep reinforcement learning algorithm that uses a neural network to approximate the Q-function. Its main contribution is the use of experience replay and a target network to stabilize the training process and improve the performance of the algorithm."
  },
  {
    "id": "q1",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Medium",
    "question": "What is the main difference between on-policy and off-policy reinforcement learning algorithms?",
    "answer": "On-policy algorithms learn from experiences generated by following a specific policy, whereas off-policy algorithms learn from experiences generated by any policy, including the target policy."
  },
  {
    "id": "q2",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Medium",
    "question": "Suppose you are training an agent to play a game using Q-learning. The agent's current state is a random forest, and the action space consists of cutting, pruning, and harvesting. If the agent takes the action 'cut', it receives a reward of -10. However, cutting the tree reduces the expected future reward. What is the issue with this scenario?",
    "answer": "The agent has received a negative reward for an action that actually leads to better future rewards, which is problematic for Q-learning, as it updates the Q-value based on the immediate reward."
  },
  {
    "id": "q3",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Medium",
    "question": "What is the role of the exploration-exploitation trade-off in reinforcement learning?",
    "answer": "The exploration-exploitation trade-off is the balance between choosing an action that has been tried before (exploitation) and choosing an action that has not been tried before (exploration), in order to maximize the overall reward."
  },
  {
    "id": "q4",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Medium",
    "question": "How does the SARSA algorithm differ from Q-learning?",
    "answer": "SARSA uses the current action-value function to update the Q-value, whereas Q-learning uses the maximum action-value function. This makes SARSA an on-policy algorithm, whereas Q-learning can be either on-policy or off-policy."
  },
  {
    "id": "q5",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Medium",
    "question": "Suppose you are implementing a Deep Q-Network (DQN) for playing a game. How would you handle the issue of overestimation of Q-values?",
    "answer": "One way to handle overestimation of Q-values is to use Double Deep Q-Learning, which uses two separate networks to select actions and update Q-values, reducing the overestimation."
  },
  {
    "id": "q1",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Hard",
    "question": "Explain the difference between on-policy and off-policy Q-learning algorithms, and provide examples of when to use each.",
    "answer": "On-policy Q-learning updates the policy and value function simultaneously using the same data, whereas off-policy Q-learning updates the value function using data collected under a different policy. On-policy algorithms, such as SARSA, are suitable for situations where the agent is learning to follow a specific policy, while off-policy algorithms, such as Q-learning, are more efficient when the agent can learn from a large dataset of experiences."
  },
  {
    "id": "q2",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Hard",
    "question": "Describe the Bellman equation for a Markov Decision Process and how it relates to the optimal value function. Provide a mathematical expression.",
    "answer": "The Bellman equation is a recursive equation that relates the optimal value function Vπ(s) to the value function at the next state. It is expressed as Vπ(s) = ∑(p(s'|s, a) * (r(s, a) + γ * Vπ(s'))), where p(s'|s, a) is the transition probability, r(s, a) is the reward function, and γ is the discount factor."
  },
  {
    "id": "q3",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Hard",
    "question": "Suppose you are given a partially observable Markov Decision Process (POMDP). How would you modify the Q-learning algorithm to handle the partial observability?",
    "answer": "To handle partial observability, you would need to use a model that keeps track of the hidden state of the environment. This can be achieved by using a Hidden Markov Model (HMM) or a Partially Observable Markov Decision Process (POMDP) solver. The Q-learning algorithm would need to be modified to incorporate the estimated hidden state into the Q-function updates."
  },
  {
    "id": "q4",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Hard",
    "question": "What is the difference between DQN and Double DQN algorithms? Provide a brief comparison.",
    "answer": "DQN (Deep Q-Network) is a model-free reinforcement learning algorithm that uses a neural network to approximate the Q-function. Double DQN is a variant of DQN that uses two separate networks to estimate the Q-function and the target Q-function. This helps to reduce overestimation and improves the stability of the algorithm."
  },
  {
    "id": "q5",
    "domain": "Machine Learning",
    "topic": "Reinforcement Learning",
    "difficulty": "Hard",
    "question": "Explain the concept of exploration-exploitation trade-off in reinforcement learning. How would you implement an epsilon-greedy policy to balance exploration and exploitation?",
    "answer": "The exploration-exploitation trade-off is the dilemma in reinforcement learning where the agent must choose between exploring new actions to gather more information and exploiting the current knowledge to maximize rewards. An epsilon-greedy policy is a simple approach to balance exploration and exploitation by choosing the greedy action with probability (1-ε) and a random action with probability ε. This allows the agent to trade off between exploring new actions and exploiting the current knowledge."
  }
]