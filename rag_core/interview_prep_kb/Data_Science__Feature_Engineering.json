[
  {
    "id": "q1",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Easy",
    "question": "What is the main goal of feature engineering in a machine learning project?",
    "answer": "The main goal of feature engineering is to create new features from existing data that can improve the performance of a machine learning model."
  },
  {
    "id": "q2",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Easy",
    "question": "Consider a dataset with a categorical feature 'color' with values 'red', 'green', and 'blue'. How can you convert this feature into a numerical feature?",
    "answer": "You can use one-hot encoding to convert the categorical feature 'color' into a numerical feature. This involves creating three new binary features, one for each color, where the value is 1 if the color matches and 0 otherwise."
  },
  {
    "id": "q3",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Easy",
    "question": "A dataset has a feature 'age' in years. How can you convert this feature into a feature that represents the age group?",
    "answer": "You can use binning to convert the 'age' feature into an age group feature. This involves dividing the age range into groups, such as 0-20, 21-40, 41-60, and above 60."
  },
  {
    "id": "q4",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Easy",
    "question": "What is the benefit of using polynomial features in a machine learning model?",
    "answer": "The benefit of using polynomial features is that it can help to capture non-linear relationships between the features and the target variable, which can improve the performance of the model."
  },
  {
    "id": "q5",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Easy",
    "question": "Consider a dataset with a feature 'temperature' that has missing values. How can you handle these missing values?",
    "answer": "You can use imputation techniques, such as mean or median imputation, to replace the missing values with the mean or median value of the feature."
  },
  {
    "id": "q1",
    "domain": "DataScience",
    "topic": "FeatureEngineering",
    "difficulty": "Medium",
    "question": "What is the difference between a feature and a target variable?",
    "answer": "A feature is a variable used for prediction, while a target variable is the variable being predicted."
  },
  {
    "id": "q2",
    "domain": "DataScience",
    "topic": "FeatureEngineering",
    "difficulty": "Medium",
    "question": "How would you handle missing values in a dataset for feature engineering?",
    "answer": "You can use imputation techniques such as mean/median/mode substitution, regression-based imputation, or more advanced methods like K-Nearest Neighbors (KNN) or MICE."
  },
  {
    "id": "q3",
    "domain": "DataScience",
    "topic": "FeatureEngineering",
    "difficulty": "Medium",
    "question": "What are some common techniques used to transform categorical variables into numerical features?",
    "answer": "Common techniques include one-hot encoding, label encoding, and ordinal encoding."
  },
  {
    "id": "q4",
    "domain": "DataScience",
    "topic": "FeatureEngineering",
    "difficulty": "Medium",
    "question": "How do you decide which features to include in a model and which to exclude?",
    "answer": "You can use techniques like recursive feature elimination (RFE), mutual information, or permutation importance to select the most relevant features."
  },
  {
    "id": "q5",
    "domain": "DataScience",
    "topic": "FeatureEngineering",
    "difficulty": "Medium",
    "question": "What is the difference between polynomial features and interaction features?",
    "answer": "Polynomial features are created by combining multiple features using a polynomial function, while interaction features are created by combining two or more features to capture their joint effect on the target variable."
  },
  {
    "id": "q1",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Hard",
    "question": "You have a dataset with a categorical feature that has a large number of categories. How would you handle this feature for a machine learning model?",
    "answer": "One way to handle a large categorical feature is to use dimensionality reduction techniques such as label encoding or one-hot encoding. However, these methods can lead to the curse of dimensionality. A better approach is to use techniques like target encoding or mean encoding, which can capture more information about the feature. Another option is to use a library like Category Encoders, which provides various encoding methods for categorical features."
  },
  {
    "id": "q2",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Hard",
    "question": "A machine learning model is performing well on a subset of the data, but poorly on the rest. How would you investigate and address this issue?",
    "answer": "To investigate the issue, I would first check for data leakage by verifying that no information from the test set is being used in the training process. Next, I would analyze the distribution of the target variable and the features to identify any patterns or correlations that could be causing the poor performance. I would also check for outliers and missing values in the data. To address the issue, I might consider using techniques like data augmentation, feature selection, or ensemble methods to improve the model's performance."
  },
  {
    "id": "q3",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Hard",
    "question": "Suppose you have two features, X and Y, that are highly correlated with each other. How would you decide which feature to use in a machine learning model?",
    "answer": "To decide which feature to use, I would consider the following factors: (1) the strength of the correlation between X and Y, (2) the interpretability of the features, (3) the importance of the features in the model, and (4) the potential for multicollinearity. A general rule of thumb is to use the feature that has the highest correlation with the target variable or is more interpretable. However, in some cases, it may be beneficial to use both features or to combine them in some way to capture more information."
  },
  {
    "id": "q4",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Hard",
    "question": "You have a dataset with a feature that has a non-linear relationship with the target variable. How would you create a new feature to capture this non-linear relationship?",
    "answer": "One way to capture a non-linear relationship is to use polynomial transformations or interactions between the feature and other features. For example, you could create a new feature by squaring the original feature or by interacting it with another feature. Another option is to use a non-linear transformation like the log or exponential function. Additionally, techniques like Fourier transforms or wavelet transforms can be used to capture non-linear relationships."
  },
  {
    "id": "q5",
    "domain": "Data Science",
    "topic": "Feature Engineering",
    "difficulty": "Hard",
    "question": "A machine learning model is performing well, but the feature importance is not making sense. How would you investigate and address this issue?",
    "answer": "To investigate the issue, I would first check the feature importance scores to see if they are consistent across different algorithms or models. Next, I would analyze the relationships between the features and the target variable to identify any patterns or correlations that could be causing the misleading feature importance. I would also check for multicollinearity and feature redundancy. To address the issue, I might consider using techniques like recursive feature elimination, mutual information, or information gain to select more informative features."
  }
]